<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pavan Ramkumar Research Portfolio</title>

    <!-- Bootstrap Core CSS -->
    <!--<link href="css/bootstrap.min.css" rel="stylesheet"> -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/grayscale.css" rel="stylesheet">

    <!-- Favicon -->
    <link href="figs/favicon-bar-chart-o.ico" rel="shortcut icon">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- D3JS scripts -->
    <script type="text/javascript" src="./d3/d3.v3.js"></script>

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <i class="fa fa-play-circle"></i>  <span class="light">Home</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publications">Publications</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
                            Projects
                            <b class="caret"></b>
                        </a>
                        <ul class="dropdown-menu">
                            <li><a tabindex="-1" href="#projects-1">Visual search and FEF</a></a></li>
                            <li><a tabindex="-1" href="#projects-2">Gist perception and MEG</a></li>
                            <li><a tabindex="-1" href="#projects-3">Behavioral and neural correlates of movement chunking</a></li>
                            <li><a tabindex="-1" href="#projects-4">Neural representation of uncertainty</a></li>
                            <li><a tabindex="-1" href="#projects-5">Reward coding in PMd and M1</a></li>
                            <li><a tabindex="-1" href="#projects-6">Neural representation of color in active vision</a></li>
                            <li><a tabindex="-1" href="#projects-7">Synthetic neurophysiology with deep networks</a></li>
                        </ul>
                    </li>
                    <li>
                        <a class="page-scroll" href="#collaborators">Collaborators</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <p></p>
                        <p></p>
                        <p></p>
                        
                        <h6 class="brand-heading">Building Marr's bridges</h6>
                        
                        <p class="intro-text">To understand the relationship between behavior and the brain one has to begin by defining the function, or the computational goal, of a complete behavior. Only then can a neuroscientist determine how the brain achieves that goal – David Marr </p>
                        
                        <a href="#about" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <figure>
                    <img src="figs/Pavan.jpg" alt="Pavan" width=400 class="img-rounded" >
                    <figcaption>Photo Credit: Titipat Achakulvisut</figcaption>
                </figure>
                <h2>Pavan Ramkumar</h2>

                <ul class="list-inline banner-social-buttons">
                    <li>
                        <a href="mailto:pavan.ramkumar@gmail.com" class="btn btn-default btn-lg"><i class="fa fa-envelope-o fa-fw"></i> <span class="network-name">Email</span></a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=JtltLUAAAAAJ&hl=en" class="btn btn-default btn-lg"><i class="fa fa-graduation-cap fa-fw"></i> <span class="network-name">Scholar</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/blob/master/pdfs/PavanRamkumar_CV_Jan_2016.pdf" class="btn btn-default btn-lg"><i class="fa fa-file-text-o fa-fw"></i> <span class="network-name">CV</span></a>
                    </li>
                    <li>
                        <a href="https://twitter.com/desipoika" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Twitter</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/pavanramkumar" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">Github</span></a>
                    </li>
                </ul>

                <p>Postdoc | Kording Lab | Northwestern University</p>
                
                <p>The long-term goal of my research is to reveal (1) the motivations underlying our behaviors, (2) the strategies we use to achieve them, and (3) the biophysical mechanisms that enable these strategies.</p>
                <p>I specialize in computational modeling of behavior and neural data analysis, bringing insights from electrical engineering, machine learning, and statistics. I am experienced in some of the most widespread techniques — primate neurophysiology, human electromagnetic imaging (EEG/MEG), and functional MRI — deployed to study neural systems across scales of ~100 to ~1,000,000 neurons. I have applied these tools to study the somatosensory, visual, eye-movement, motor control, and reward systems in the brain.</p>
            </div>
        </div>
    </section>

    <!-- Publications Section -->
    <section id="publications" class="container content-section text-justify">
        
        <!-- List the publications -->
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>In Preparation</h2>
                <p>[16] <span class="author">Ramkumar P</span>, Acuna DE, Berniker M, Grafton S, Turner RS, Körding KP. Optimization costs underlying movement sequence chunking in basal ganglia. </p>
                <p>[15] <span class="author">Ramkumar P</span>, Fernandes HL, Smith MA, Körding KP. Hue tuning during active vision in natural scenes. </p>
                <p>[14] <span class="author">Ramkumar P</span>, Cooler S, Dekleva BM, Miller EL, Körding KP. A reinforcement signal in motor and premotor cortices. </p>

                <h2>Under Review</h2>
                <p>[13] Glaser JI*, Wood DW*, Lawlor PN, <span class="author">Ramkumar P</span>, Körding KP, Segraves MA. Frontal eye field represents expected reward of saccades during natural scene search. </p>
                <p>[12] Dekleva BM, <span class="author">Ramkumar P</span>, Wanda PA, Körding KP, Miller LE. Uncertainty leads to persistent representations of alternative movements in PMd. </p>

                <h2>Under Revision</h2>
                <p>[11] <span class="author">Ramkumar P</span>, Acuna DE, Berniker M, Grafton S, Turner RS, Körding KP. Chunking as the result of an efficiency–computation tradeoff. <span class="journal">Nature Communications</span>.</p>
                <p>[10] <span class="author">Ramkumar P</span>, Hansen BC, Pannasch S, Loschky LC. Visual information representation and natural scene categorization are simultaneous across cortex: An MEG study. <span class="journal">Neuroimage</span>.</p>
                <p>[9] <span class="author">Ramkumar P*</span>, Lawlor PN*, Glaser JI, Wood DW, Segraves MA, Körding KP. Feature-based attention and spatial selection in frontal eye fields during natural scene search. <span class="journal">Journal of Neurophysiology</span>. </p>

                <h2>2015</h2>
                <p>[8] <span class="author">Ramkumar P</span>, Fernandes HL, Körding KP, Segraves MA. 2015. Modeling peripheral visual acuity enables discovery of gaze strategies at multiple time scales during natural scene search. <span class="journal">Journal of Vision</span>, 15(3):19. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/08-Ramkumar_etal_JVis_2015.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>

                <h2>2014</h2>
                <p>[7] <span class="author">Ramkumar P</span>, Parkkonen L, Hyvärinen A. 2014. Group-level spatial independent component analysis of Fourier envelopes of resting-state MEG data. <span class="journal">Neuroimage</span>, 86:480–491. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/07-Ramkumar_etal_Neuroimage_2014.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>

                <h2>2013</h2>
                <p>[6] <span class="author">Ramkumar P</span>, Jas M, Pannasch S, Parkkonen L, Hari R. 2013. Feature-specific information processing precedes concerted activation in human visual cortex. <span class="journal">Journal of Neuroscience</span>, 33: 7691–7699. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/06-Ramkumar_etal_JNeurosci_2013.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>
                <p>[5] Hyvärinen A, <span class="author">Ramkumar P</span>. 2013. Testing independent component patterns by inter-subject or inter-session consistency. <span class="journal">Frontiers in Human Neuroscience</span>, 7 (94). <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/05-Hyvarinen_Ramkumar_Frontiers_2013.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>

                <h2>2012</h2>
                <p>[4] <span class="author">Ramkumar P</span>, Parkkonen L, Hari R, Hyvärinen A. 2012. Characterization of neuromagnetic brain rhythms over time scales of minutes using spatial independent component analysis. <span class="journal">Human Brain Mapping</span>, 33: 1648–1662. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/04-Ramkumar_etal_HBM_2012.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>

                <h2>2010</h2>
                <p>[3] Hyvärinen A, <span class="author">Ramkumar P</span>, Parkkonen L, Hari R. 2010. Independent component analysis of short-time Fourier transforms for spontaneous EEG/MEG analysis. <span class="journal">Neuroimage</span>, 49: 257–271. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/03-Hyvarinen_etal_Neuroimage_2010.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>
                <p>[2] <span class="author">Ramkumar P</span>, Parkkonen L, Hari R. 2010. Oscillatory Response Function: Towards a parametric model of rhythmic brain activity. <span class="journal">Human Brain Mapping</span>, 31: 820–834. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/02-Ramkumar_etal_HBM_2010.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>
                <p>[1] Malinen S, Vartiainen N, Hlushchuk Y, Koskinen M, <span class="author">Ramkumar P</span>, Forss N, Kalso E, Hari R. 2010. Aberrant spatiotemporal resting-state brain activation in patients with chronic pain. <span class="journal">Proceedings of the National Academy of Sciences USA</span>, 107: 6493–6497. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/01-Malinen_etal_PNAS_2010&suppl.pdf"><i class="fa fa-file-pdf-o"></i> [pdf]</a></p>
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-1" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Visual search and FEF</h2>
                <h4><span style="color: #3b5998">With: Hugo Fernandes, Pat Lawlor, Josh Glaser, Daniel Wood, Mark Segraves, Konrad Körding</span></h4>

                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second. Deciding where to look next among a large number of alternatives is thus one of the most frequent decisions we make. Why do we look where we look? </p>

                <p>Studies have shown that both bottom-up (e.g. luminance contrast, saliency, edge–energy) and top-down factors (such as target similarity or relevance) influence the guidance of eye movements. Predictive models of eye movements are derived from priority maps composed of one or more of these factors. Evidence for such maps have been reported in the lateral intra parietal (LIP) cortex, the frontal eye field (FEF), primary visual cortex (V1) and/or ventral visual area V4, but computational maps of priority have not been used to model neural activity. </p>

                <p>In this project, we attempt to unify computational models and neurophysiology of gaze priority maps. We developed primate models of gaze behavior in natural scenes by rewarding monkeys to find targets embedded in scenes. We build predictive models of gaze using computational definitions of visual priority and quantify model predictions on monkeys' fixation choices. </p>

                <figure>
                    <img src="figs/FEF_Figure01.png" alt="FEF" width=750 class="img-responsive" >
                    <figcaption>
                        Fig. 1. Prediction of gaze using visual features at fixation. We compared bottom-up saliency, top-down relevance and edge-energy at fixated (above: left panel) and non-fixated, i.e. shuffled control (above: right panel) targets by computing the area under the ROC curves (below). The star indicates statistically significant difference from a chance level of 0.5.
                    </figcaption>
                </figure>

                <p> To ask if FEF neurons represent computational descriptions of priority including saliency, relevance and energy, we build generalized linear models (GLMs) of Poisson-spiking neurons (see Fig. 2). </p>

                <figure>
                    <img src="figs/FEF_Figure02.png" alt="FEF" width=750 class="img-responsive" >
                    <figcaption>
                        Fig. 2. A comprehensive generative model of neural spikes using a GLM framework. The model comprises visual features: saliency, relevance and energy from a neighborhood around fixation location after the saccade, un-tuned responses aligned to saccade and fixation onsets, and the direction of the saccade. The features are passed through parameterized spatial filters (representing the receptive field) and temporal filters. The model also comprises spike history terms (or self terms). All these features are linearly combined followed by an exponential nonlinearity, which gives the conditional intensity function of spike rate, given model parameters. Spikes are generated from this model by sampling from a Poisson distribution with mean equal to the conditional intensity function. Brown: basis functions modeling temporal activity around the saccade onset; Green: basis functions modeling temporal responses around the fixation onset; Blue: basis functions modeling temporal responses after spike onset.
                    </figcaption>
                </figure>
                
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-2" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Rapid Scene Categorization and MEG</h2>
                <h4><span style="color: #3b5998">With: Sebastian Pannasch, Bruce Hansen, Lester Loschky</span></h4>

                <p>To make effective decisions in our environment, our brains must be able to effectively recognize and comprehend real-world scenes. Human are remarkable at recognizing scene categories from the briefest of glimpses (< 20 ms). The holistic information that can be extracted in such short durations has come to be known as <b><i>scene gist</i></b>.</p>

                <p>Computational models of scene gist, such as the spatial envelope (SpEn) model, as well as behavioral studies provide useful suggestions for what visual features the brain might use to categorize scenes. However, they do not inform us about when and where in the brain such information is represented and how scene-categorical judgments are made on the basis of these representations.</p>

                <img src="figs/MEGFigure01.png" alt="MEG" width=750 class="img-responsive" >
                <p>Here, we investigate the brain-behavior relationship underlying rapid scene categorization. We use whole-scalp magnetoencephalography (MEG) to track visual scene information flow in the ventral and temporal cortex, using spatially and temporally resolved maps of decoding accuracy. To investigate the time course of visual representation versus behavioral category judgment, we then use neural decoders in concert with decoders based on SpEn features to study errors in behavioral categorization. Using confusion matrices, we tracked how well patterns of errors in neural decoders could be explained by SpEn decoders and behavioral errors. We find that both SpEn decoders and behavioral errors explain unique variance throughout the ventrotemporal cortex, and that their effects are temporally simultaneous and restricted to 100-250 ms after stimulus onset. Thus, during rapid scene categorization, neural processes that ultimately result in behavioral categorization are simultaneous and colocalized with neural processes underlying visual information representation. </p>

            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-3" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Tradeoffs in movement sequence learning and the role of basal ganglia</h2>
                <h4><span style="color: #3b5998">With: Daniel Acuna, Max Berniker, Rob Turner, Scott Grafton, Konrad Körding</span></h4>

                <p>We routinely execute complex movement sequences with such effortless ease that the cost of planning them optimally is often under-appreciated. When movements are learned from external cues, they start out highly regular, progressively become more varied until the become habitual and more regular again. </p>
                <img src="figs/Chunking_Figure01.png" alt="Chunking" width=750 class="img-responsive" >

                <p> A common facet of many such complex movements is that they tend to be discrete nature, i.e. they are often executed as chunks. Here, we framed movement chunking as the result of a trade-off between the desire to make efficient movements and minimize the computational complexity of optimizing them. We show that monkeys adopt a cost-effective strategy to deal with this tradeoff. By modeling chunks as minimum-jerk trajectories, we found that kinematic sequences are best described as progressively resembling locally optimal trajectories, with optimization occurring within chunks. Thus, the cumulative optimization costs are kept in check over the course of learning.</p>
                <img src="figs/Chunking_Figure02.png" alt="Chunking" width=750 class="img-responsive" >

                <p>We also record from globus pallidus (GP) — motor output structures in the basal ganglia — and show that GP neurons encode expected complexity of optimizing movements. Optimal control is thus an important driver of movement sequence learning, both behaviorally and neurally. </p>
                <img src="figs/Chunking_Figure03.png" alt="Chunking" width=750 class="img-responsive" >

            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-4" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Neural representation of subjective uncertainty in PMd during reaching</h2>
                <h4><span style="color: #3b5998">With: Brian Dekleva, Paul Wanda, Lee Miller, Konrad Körding</span></h4>

                <p>Movements in the real world are often planned with uncertain information about where to move. Understanding the role of uncertainty in movement plans is important to improve rehabilitation therapies and brain-based prostheses. Bayesian estimation theory, which combines sources of information in proportion to their uncertainty, predicts movement behavior when uncertainty about subjective beliefs (priors) and sensory observations (likelihoods) is varied. By manipulating sensory uncertainty of the target in each trial of a reaching task, as well as the prior uncertainty of the target location during the task, we show that monkeys' reaches can be predicted using a Bayesian model that weights the different sources of uncertainty appropriately. </p>
                <img src="figs/Uncertainty_Figure01.jpg" alt="FEF" width=750 class="img-responsive" >

                <p> To perform probabilistic inference of this nature, the brain must represent/reflect uncertainty. We asked how sensory uncertainty is represented in population activity in dorsal premotor cortex (PMd) and primary motor cortex (M1) during movement preparation. We found that greater subjective uncertainty, led to incresed firing rates and broad recruitment of neurons in PMd but not M1. This broad recruitment suggests that multiple movement plans are represented in PMd activity until uncertainty-reducing feedback about the target location is received. </p>

            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-5" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Reward signaling in premotor and motor cortices</h2>
                <h4><span style="color: #3b5998">With: Brian Dekleva, Sam Cooler, Lee Miller, Konrad Körding</span></h4>
                <p>Reward is an important feedback signal for motor learning. How reward information reaches the motor cortex to influence movement planning and execution is unknown. Therefore, we asked whether premotor and motor cortices encode reward. We found a strong and robust encoding of reward outcome in premotor (PMd) and motor (M1) cortices. In particular, neurons increased their firing rate following trials that were not rewarded. We further investigated the nature of this signal and established that it is unlike any previously reported reward signal in the brain. It is unrelated to reward magnitude expectation or prediction error, it is not influenced by history of reward, and it is not modulated by error magnitude. Using generalized linear modeling of spikes we also carefully verified that the signal is not explained away by differences in kinematics or return reach planning activity. Thus, we found a categorical reward signal in PMd and M1 signaling the presence or absence of reward at the end of a goal-directed task.</p>
                <img src="figs/Reward_Figure01.png" alt="Reward" width=750 class="img-responsive" >
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-6" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Modeling color tuning of V4 neurons during active natural vision</h2>
                <h4><span style="color: #3b5998">With: Hugo Fernandes, Matt Smith, Konrad Körding</span></h4>
                <p>Hue is a circular variable</p>
                <img src="figs/V4_Figure01.png" alt="V4" width=750 class="img-responsive" >

                <p>We built a cool GLM </p>
                <img src="figs/V4_Figure02.png" alt="V4" width=750 class="img-responsive" >

                <p>We can predict firing rates with feature-temporal models of hue. </p>
                <img src="figs/V4_Figure03.png" alt="V4" width=750 class="img-responsive" >

            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-7" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Synthetic Neurophysiology with Deep Convolutional Networks</h2>
                <h4><span style="color: #3b5998">With: Hugo Fernandes, Matt Smith, Konrad Körding</span></h4>
                <p>Deep convolutions neural networks (CNNs) have recently seen tremendous success in real-world visual tasks such as object recognition. It has also been shown that they capture the representational properties of the inferior temporal cortex (IT) in both humans and monkeys across a wide range of object categories, suggesting that they are excellent biophysical and computational approximations of the feed-forward part of the ventral visual stream. Thus, pre-trained CNNs can be used as substrates to study the functional properties of neurons all over the ventral stream. This is a promising new technique to bridge computational and physiological approaches to understanding vision and has come to be known as synthetic neurphysiology. </p>
                <img src="figs/DeepCNN_Figure01.png" alt="V4" width=750 class="img-responsive" >

                <p>Here, we focus on the visual area V4. We present artificial stimuli such as colored checkerboards (picture above) to CNNs and study the tuning properties of synthetic neurons in the CNN. Preliminary analysis suggests that synthetic neurons are hue-tuned, and further, the distribution of preferred hue across the population of synthetic neurons resembles those of real V4 neurons characterized during the free-viewing of natural scenes. Both these distributions also mirror the highly non-uniform distribution of hues in natural scenes. </p>
            </div>
        </div>
    </section>


    <section id="collaborators" class="container content-section text-justify">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Graph visualization of collaborators and projects</h2>
                <p> 
                    Below is a visualization of my publications and ongoing projects embedded in my network of 28 co-authors using the <a href="https://github.com/mbostock/d3/wiki/Force-Layout">force-directed graph</a> schema. Cool-colored nodes represent authors and warm-colored nodes represent projects. Links represent authors that have collaborated together on a project. Nodes representing co-authors are sized according to the number of papers or projects I have in common with them. If you're viewing this on a desktop, moving the mouse over a node gives you the name of the author or the title of the publication or project that it represents.
                
                
                 <table style="width:100%">
                  <tr>
                    <td>[<span style="color:#07b849"><i class="fa fa-circle"></i></span>]</td>
                    <td>Me</td>
                    <td>[<span style="color:#3b5998"><i class="fa fa-circle"></i></span>]</td>
                    <td>PI co-authors</td>
                    <td>[<span style="color:#88bee4"><i class="fa fa-circle"></i></span>]</td>
                    <td>Grad student or postdoc co-authors</td>
                  </tr>
                  <tr>
                    <td>[<span style="color:#9e9225"><i class="fa fa-circle"></i></span>]</td>
                    <td>Published Articles</td>
                    <td>[<span style="color:#d2c43d"><i class="fa fa-circle"></i></span>]</td>
                    <td>Articles Under Review/ Revision</td>
                    <td>[<span style="color:#d27a3d"><i class="fa fa-circle"></i></span>]</td>
                    <td>Ongoing Projects</td>
                  </tr>
                </table> 

                </p>

                <!-- Add d3js visualization  -->

                <script>

                    var width = 1250,
                        height = 500;

                    var color = d3.scale.category20();

                    var force = d3.layout.force()
                        .charge(-150)
                        .linkDistance(70)
                        .gravity(0.05)
                        .size([width, height]);

                    var svg = d3.select("body").append("svg")
                        .attr("width", width)
                        .attr("height", height);

                    d3.json("./d3/projects_pavan_nocomments.json", function(error, graph) {
                      force
                          .nodes(graph.nodes)
                          .links(graph.links)
                          .start();

                      var link = svg.selectAll("line.link")
                          .data(graph.links)
                        .enter().append("line")
                          .attr("class", "link")
                          .style("stroke-width", function(d) { return Math.sqrt(d.value); });

                      var node = svg.selectAll("circle.node")
                          .data(graph.nodes)
                        .enter().append("circle")
                          .attr("class", "node")
                          .attr("r", function(d) { return d.size; })
                          .style("fill", function(d) { return d.color; })
                          .call(force.drag);

                      node.append("title")
                          .text(function(d) { return d.name; });

                      force.on("tick", function() {
                        link.attr("x1", function(d) { return d.source.x; })
                            .attr("y1", function(d) { return d.source.y; })
                            .attr("x2", function(d) { return d.target.x; })
                            .attr("y2", function(d) { return d.target.y; });

                        node.attr("cx", function(d) { return d.x; })
                            .attr("cy", function(d) { return d.y; });
                      });
                    });

                </script>
            </div>
        </div>
    </section>

    <!-- Map Section
    <div id="map"></div> -->

    <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>&copy; Pavan Ramkumar 2016</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/jquery.easing.min.js"></script>

    <!-- <!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/
    <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script> -->

    <!-- Custom Theme JavaScript -->
    <script src="js/grayscale.js"></script>

</body>

</html>
